{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TXP5nalr9ls"
      },
      "source": [
        "# **My Tokenizer**\n",
        "\n",
        "Beyza Balota - 31232\n",
        "\n",
        "20.10.2024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def my_tokenizer(corpus_raw):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus to be tokenized\n",
        "    rtype: list\n",
        "    return: a list of tokens extracted from the corpus_raw\n",
        "    '''\n",
        "\n",
        "    # Step 1: Remove content inside square brackets (e.g., [t], [+2])\n",
        "    corpus_raw = re.sub(r\"\\[.*?\\]\", \"\", corpus_raw)\n",
        "\n",
        "    # Step 2: Remove unnecessary symbols like ##\n",
        "    corpus_raw = re.sub(r\"##\", \"\", corpus_raw)\n",
        "\n",
        "    # Step 3: Handle punctuation and words\n",
        "    # Find all sequences of alphanumeric characters, including hyphenated words, or separate punctuation marks\n",
        "    token_list = re.findall(r\"\\w+(?:-\\w+)*|[.,!?;()\\\"']\", corpus_raw)\n",
        "\n",
        "    # Step 4: Handle common contractions (e.g., don't -> do not)\n",
        "    token_list = [re.sub(r\"n't\", \" not\", token) for token in token_list]\n",
        "    token_list = [re.sub(r\"'re\", \" are\", token) for token in token_list]\n",
        "    token_list = [re.sub(r\"'s\", \"'is\", token) for token in token_list]  \n",
        "    token_list = [re.sub(r\"'ve\", \" have\", token) for token in token_list]\n",
        "    token_list = [re.sub(r\"'ll\", \" will\", token) for token in token_list]\n",
        "\n",
        "    # Step 5: Convert tokens to lowercase\n",
        "    token_list = [token.lower() for token in token_list]\n",
        "\n",
        "    # Step 6: Filter out any empty strings or stray characters\n",
        "    token_list = [token for token in token_list if token.strip()]\n",
        "\n",
        "    return token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Choosing and Dowloading the Corpus & Calling my Tokenizer Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-ZF4WJjKxZfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package product_reviews_2 to\n",
            "[nltk_data]     /Users/beyzabalota/nltk_data...\n",
            "[nltk_data]   Package product_reviews_2 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#main code\n",
        "\n",
        "#using nltk to download my selected corpus\n",
        "import nltk \n",
        "\n",
        "corpus_name = 'product_reviews_2'\n",
        "\n",
        "#download the corpus and import it.\n",
        "nltk.download('product_reviews_2')\n",
        "from nltk.corpus import product_reviews_2\n",
        "\n",
        "#get the raw text output of the corpus to the corpus_raw variable\n",
        "corpus_raw = product_reviews_2.raw()\n",
        "\n",
        "#call tokenizer method\n",
        "my_tokenized_list = my_tokenizer(corpus_raw)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Displaying Raw and Tokenized Versions of the Corpus \n",
        "(decoding purposes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Original raw text:'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\"[t]\\nSD500[+2]##We really enjoyed shooting with the Canon PowerShot SD500. \\ndesign[+2]##It has an exterior design that combines form and function more elegantly than any point-and-shoot we've ever test\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "'Tokenized output:'"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['sd500we',\n",
              " 'really',\n",
              " 'enjoyed',\n",
              " 'shooting',\n",
              " 'with',\n",
              " 'the',\n",
              " 'canon',\n",
              " 'powershot',\n",
              " 'sd500',\n",
              " '.',\n",
              " 'designit',\n",
              " 'has',\n",
              " 'an',\n",
              " 'exterior',\n",
              " 'design',\n",
              " 'that',\n",
              " 'combines',\n",
              " 'form',\n",
              " 'and',\n",
              " 'function']"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#displaying for decoding purposes\n",
        "from IPython.display import display\n",
        "\n",
        "# Output the first 200 characters of the raw text\n",
        "display(\"Original raw text:\", corpus_raw[:200])\n",
        "\n",
        "# Output the first 20 tokens from the tokenized list\n",
        "display(\"Tokenized output:\", my_tokenized_list[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuTDStwY36cT"
      },
      "source": [
        "## Please do not touch the code below that will evaluate your tokenizer with the nltk word tokenizer. You will get zero points from evaluation if you do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8txzw8Ag8ysD"
      },
      "outputs": [],
      "source": [
        "def similarity_score(set_a, set_b):\n",
        "    '''\n",
        "    type set_a: set\n",
        "    param set_a: The first set to be compared\n",
        "    type set_b: set\n",
        "    param set_b: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: similarity score with two sets using Jaccard similarity.\n",
        "    '''\n",
        "\n",
        "    jaccard_similarity = float(len(set_a.intersection(set_b)) / len(set_a.union(set_b)))\n",
        "\n",
        "    return jaccard_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6wUTqReb36Hg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/beyzabalota/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import punkt\n",
        "\n",
        "def evaluation(corpus_raw, token_list):\n",
        "    '''\n",
        "    type corpus_raw: string\n",
        "    param corpus_raw: The raw output of the corpus\n",
        "    type token_list: list\n",
        "    param token_list: The tokens extracted from the corpus_raw\n",
        "    rtype: float\n",
        "    return: comparison score with the given token list and the nltk tokenizer.\n",
        "    '''\n",
        "\n",
        "    #The comparison score only looks at the tokens but not the frequencies of the tokens.\n",
        "    #we assume case folding is already applied to the token_list\n",
        "    corpus_raw = corpus_raw.lower()\n",
        "    nltk_tokens = word_tokenize(corpus_raw, language='english')\n",
        "\n",
        "    score = similarity_score(set(token_list), set(nltk_tokens))\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Vt_uSBF-NHm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The similarity score is 0.80\n"
          ]
        }
      ],
      "source": [
        "#Evaluation\n",
        "\n",
        "eval_score = evaluation(corpus_raw, my_tokenized_list)\n",
        "\n",
        "print('The similarity score is {:.2f}'.format(eval_score))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. Report**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Corpus Name and Selection Reason\n",
        "For this assignment, I selected the product_reviews_2 corpus from the NLTK library. The reason behind this selection was that product reviews generally contain a mix of everyday language, domain-specific terminology, abbreviations, and punctuation. This makes it an interesting case for tokenization, especially as reviews may also include numbers, hyphenated terms, and contractions. I believed that this corpus would be a challenge for me to create a custom tokenizer \n",
        "\n",
        "## Design of the Tokenizer\n",
        "I have designed the tokenizer using the top-down approach. Firstly, I started with downloading the corpus using the NLTK library, then I displayed the first 200 characters to see the behaviour of the corpus, meaning that I could see unnecessary punctions or braces and get rid of them at the earlier stage. After displaying the corpus, I realized that there are many characters that are inside the square brackets and making noise inside the text, so I removed them. Furthermore, I removed some repeated symbols like “##” that were assumed unnecessary for tokenization.  I decided not to separate hyphenated words (e.g., \" point-and-shoot\") since breaking them might cause losing their original meaning. Later on, I decided to handle alphanumeric characters to ensure that words, product names, and numbers are tokenized correctly, such as \"Canon123\" or \"camera. I also handled punctuation in this part by separating them from the words, which is a common practice in tokenization. The reason behind that separation for me was mainly about achieving more accurate parsing and token matching. After that, I continued with contradiction handling, which I found out after some research that expanding them such as “don’t” to “do not” and “you’re” to “you are” was an excellent approach for maintaining the meaning of the words.  The final step was lowercasing for case insensitivity, which was also mentioned by Dilara Hoca in her e-mail, and removing empty tokens.\n",
        "\n",
        "## Challenges Faced During Tokenization\n",
        "During this project, I faced some difficulties. My primary challenge was handling the balance between splitting punctuation and retaining meaning in the text. Product reviews often use punctuation in informal ways, such as ellipses (...) or multiple exclamation marks (!!!). Deciding whether to keep these as individual tokens or merge them with words was tricky. Another challenge was managing hyphenated words and numeric values. As been discussed before, I aimed to keep hyphenated words intact, but there are edge cases where a more advanced algorithm could better decide whether the hyphen is meaningful or not.\n",
        "\n",
        "## Limitations of the Approach & Possible Improvements\n",
        "When it comes to the limitation of the approach, the handling of more complex punctuation and symbols can be considered. For example, repeated punctuation marks like ellipses (...) are still treated as three separate tokens. The treatment of hyphenated words is basic; it works well for many cases but could be improved with more context awareness. Additionally, expanding contractions might not always work perfectly, especially with unusual uses or variations in how people speak. The tokenizer also doesn't understand the context, so it can't adjust its behavior based on the specific content (for example, treating specialized terms differently from everyday words). Possible improvements can be made, especially in the case of contextual tokenization using machine learning models. Other than that, the tokenizer could be enhanced to deal with abbreviations and product specific terminology more effectively, such as recognizing that \"25mm\" or \"3D\" are some specific meaningful terms that should not be split.\n",
        "\n",
        "## Why This Approach May Be Better than NLTK\n",
        "While NLTK's word tokenizer is widely used, it does not handle some contractions and hyphenated words as effectively as the tokenizer I have implemented in my project. My approach makes deliberate efforts to expand contractions, handle possessive cases, and manage specific terms like hyphenated words and numbers more carefully. This results in more contextually accurate tokens that reflect the true meaning of product reviews. However, the evaluation score is based on similarity to NLTK’s tokenizer, which limits the score despite these enhancements. I got 0.80 similarity, which is quite good. Despite this, my approach could be better for future tasks. \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
